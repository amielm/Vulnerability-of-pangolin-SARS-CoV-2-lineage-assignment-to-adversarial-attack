{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data using the (modified) pangolin pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(cmd):\n",
    "    parser = argparse.ArgumentParser(description='pangoLEARN.')\n",
    "    parser.add_argument(\"--header-file\", action=\"store\", type=str, dest=\"header_file\")\n",
    "    parser.add_argument(\"--model-file\", action=\"store\", type=str, dest=\"model_file\")\n",
    "    parser.add_argument(\"--fasta\", action=\"store\", type=str, dest=\"sequences_file\")\n",
    "    parser.add_argument(\"--reference-file\",action=\"store\",type=str,dest=\"reference_file\")\n",
    "    parser.add_argument(\"-o\",\"--outfile\", action=\"store\", type=str, dest=\"outfile\")\n",
    "    return parser.parse_args(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_args(cmd):\n",
    "    parser = argparse.ArgumentParser(description='pangoLEARN.')\n",
    "    parser.add_argument(\"--header-file\", action=\"store\", type=str, dest=\"header_file\")\n",
    "    parser.add_argument(\"--model-file\", action=\"store\", type=str, dest=\"model_file\")\n",
    "    parser.add_argument(\"--fasta\", action=\"store\", type=str, dest=\"sequences_file\")\n",
    "    parser.add_argument(\"--reference-file\",action=\"store\",type=str,dest=\"reference_file\")\n",
    "    parser.add_argument(\"-o\",\"--outfile\", action=\"store\", type=str, dest=\"outfile\")\n",
    "    return parser.parse_args(cmd)\n",
    "\n",
    "cmd_options = \"\"\"--header-file\n",
    ".../envs/pangolin-10.10.21/lib/python3.8/site-packages/pangoLEARN/data/decisionTreeHeaders_v1.joblib\n",
    "--model-file\n",
    ".../envs/pangolin-10.10.21/lib/python3.8/site-packages/pangoLEARN/data/decisionTree_v1.joblib\n",
    "--reference-file\n",
    ".../envs/pangolin-10.10.21/lib/python3.8/site-packages/pangolin/data/reference.fasta\n",
    "--fasta\n",
    ".../sequences.aln.fasta\n",
    "-o\n",
    ".../debug_classifier_output\n",
    "\"\"\"\n",
    "\n",
    "args = parse_args(cmd_options.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "referenceFile = args.reference_file\n",
    "\n",
    "referenceSeq = \"\"\n",
    "referenceId = \"reference\"\n",
    "\n",
    "def findReferenceSeq():\n",
    "    currentSeq = \"\"\n",
    "\n",
    "    with open(referenceFile) as f:\n",
    "        for line in f:\n",
    "            if \">\" not in line:\n",
    "                currentSeq = currentSeq + line.strip()\n",
    "\n",
    "    f.close()\n",
    "    return currentSeq\n",
    "# function for handling weird sequence characters\n",
    "def clean(x, loc):\n",
    "    x = x.upper()\n",
    "\n",
    "    if x == 'T' or x == 'A' or x == 'G' or x == 'C' or x == '-':\n",
    "        return x\n",
    "\n",
    "    if x == 'U':\n",
    "        return 'T'\n",
    "\n",
    "    # replace ambiguity with the reference seq value\n",
    "    return referenceSeq[loc]\n",
    "\n",
    "\n",
    "# generates data line\n",
    "def encodeSeq(seq, indiciesToKeep):\n",
    "    dataLine = []\n",
    "    for i in indiciesToKeep:\n",
    "        if i < len(seq):\n",
    "            dataLine.extend(clean(seq[i], i))\n",
    "\n",
    "    return dataLine\n",
    "\n",
    "\n",
    "# reads in the two data files\n",
    "def readInAndFormatData(sequencesFile, indiciesToKeep, blockSize=50000):\n",
    "    idList = []\n",
    "    seqList = []\n",
    "    rawSeqList = []\n",
    "\n",
    "    # open sequencesFile, which is the first snakemake input file\n",
    "    with open(sequencesFile) as f:\n",
    "        currentSeq = \"\"\n",
    "\n",
    "        # go line by line through the file, collecting a list of the sequences\n",
    "        for line in f:\n",
    "            # if the line isn't the header line\n",
    "            if \"taxon,lineage\" not in line:\n",
    "                line = line.strip()\n",
    "\n",
    "                if \">\" in line:\n",
    "                    # starting new entry, gotta save the old one\n",
    "                    if currentSeq:\n",
    "                        # yield sequence as one-hot encoded vector\n",
    "                        idList.append(seqid)\n",
    "                        seqList.append(encodeSeq(currentSeq, indiciesToKeep))\n",
    "                        rawSeqList.append(currentSeq)\n",
    "                        currentSeq = \"\"\n",
    "\n",
    "                    # this is a fasta line designating an id, but we don't want to keep the >\n",
    "                    seqid = line.strip('>')\n",
    "\n",
    "                else:\n",
    "                    currentSeq = currentSeq + line\n",
    "\n",
    "            if len(seqList) == blockSize:\n",
    "                yield idList, seqList\n",
    "                idList = []\n",
    "                seqList = []\n",
    "                rawSeqList = []\n",
    "\n",
    "        # gotta get the last one\n",
    "        idList.append(seqid)\n",
    "        seqList.append(encodeSeq(currentSeq, indiciesToKeep))\n",
    "        rawSeqList.append(currentSeq)\n",
    "\n",
    "    yield idList, seqList, rawSeqList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the list of headers the model needs.\n",
    "model_headers = joblib.load(args.header_file)\n",
    "indiciesToKeep = model_headers[1:]\n",
    "\n",
    "referenceSeq = findReferenceSeq()\n",
    "# possible nucleotide symbols\n",
    "categories = ['-','A', 'C', 'G', 'T']\n",
    "columns = [f\"{i}_{c}\" for i in indiciesToKeep for c in categories]\n",
    "refRow = [r==c for r in encodeSeq(referenceSeq, indiciesToKeep) for c in categories]\n",
    "\n",
    "print(\"loading model \" + datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "loaded_model = joblib.load(args.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_seqs = list(readInAndFormatData(args.sequences_file, indiciesToKeep))[0]\n",
    "idList, seqList, rawSeqList = all_seqs[0], all_seqs[1], all_seqs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(idList), len(rawSeqList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninque_chars = set([j for i in rawSeqList for j in set(i)])\n",
    "uninque_chars\n",
    "counts= {c:sum([i.count(c) for i in rawSeqList]) for c in uninque_chars}\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = [[r==c for r in row for c in categories] for row in seqList]\n",
    "# the reference seq must be added to everry block to make sure that the \n",
    "# spots in the reference have Ns are in the dataframe to guarentee that \n",
    "# the correct number of columns is created when get_dummies is called\n",
    "rows.append(refRow)\n",
    "idList.append(referenceId)\n",
    "\n",
    "# create a data from from the seqList\n",
    "d = np.array(rows, np.uint8)\n",
    "df = pd.DataFrame(d, columns=columns)\n",
    "\n",
    "predictions = loaded_model.predict_proba(df)\n",
    "\n",
    "\n",
    "pred_class_ind = predictions.argmax(axis=1)\n",
    "pred_conf = predictions.max(axis=1)\n",
    "pred_class = loaded_model.classes_[pred_class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pred_class)[\"C.1.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pred_class)[\"B.1.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(pred_class)[[\"A.2\", \"B.1.13\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(pred_class)[[\"B.1.39\", \"B.1.13\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries for working with the pangoLEARN decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_depths(clf):\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "            \n",
    "    return is_leaves, node_depth\n",
    "\n",
    "is_leaves, node_depth = calc_depths(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaf_classes = loaded_model.tree_.value.argmax(axis=2).flatten()\n",
    "leaf_classes[np.logical_not(is_leaves)] = -1\n",
    "leaf_classes_counts = pd.value_counts(leaf_classes)\n",
    "# leaf_classes_counts\n",
    "\n",
    "# A_class_ind = np.where(loaded_model.classes_ == A_class)[0][0]\n",
    "# B_class_ind = np.where(loaded_model.classes_ == B_class)[0][0]\n",
    "# print(f\"The index for {A_class} is {A_class_ind} and for {B_class} is {B_class_ind}\")\n",
    "# print((leaf_classes_counts[A_class_ind], leaf_classes_counts[B_class_ind]))\n",
    "\n",
    "# # find all of the leaf_ids that predict A.1\n",
    "# A_leaf_inds = np.where(leaf_classes == A_class_ind)[0]\n",
    "# B_leaf_inds = np.where(leaf_classes == B_class_ind)[0]\n",
    "\n",
    "# parent dictionary\n",
    "parent_dict = {val:ind for ind, val in enumerate(loaded_model.tree_.children_left)}\n",
    "parent_dict.update({val:ind for ind, val in enumerate(loaded_model.tree_.children_right)})\n",
    "# removing the -1 entry from the parent dictionary as it represents the children of leaves (don't exist) \n",
    "_ = parent_dict.pop(-1)\n",
    "\n",
    "def get_decision_path_node(node_id):\n",
    "    if(node_id == 0):\n",
    "        return [node_id]\n",
    "    return get_decision_path_node(parent_dict[node_id]) + [node_id]\n",
    "\n",
    "# the implementation using a loop was ~10x faster than the simple one-liner with sets...\n",
    "def get_adv_distance_between_decision_paths(source_leaf_decsion_path, target_leaf_decsion_path):\n",
    "    source_len = len(source_leaf_decsion_path)\n",
    "    target_len = len(target_leaf_decsion_path)\n",
    "    for i in range(min(source_len, target_len)):\n",
    "        if source_leaf_decsion_path[i] != target_leaf_decsion_path[i]:\n",
    "            break\n",
    "    return (target_len - i, source_len-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check whether any paths evaluate the same loci twice:\n",
    "is_leaves\n",
    "def check_whether_any_loci_is_evaluated_twice():\n",
    "    n_nodes = loaded_model.tree_.node_count\n",
    "    children_left = loaded_model.tree_.children_left\n",
    "    children_right = loaded_model.tree_.children_right\n",
    "    results = []\n",
    "    for node_id in range(n_nodes):\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        if not is_split_node:\n",
    "            node_decision_path = get_decision_path_node(node_id)\n",
    "            node_decision_path_loci = [loaded_model.tree_.feature[i]//5 for i in node_decision_path]\n",
    "            is_duplicated = len(node_decision_path_loci) != len(np.unique(node_decision_path_loci))\n",
    "#             print(is_duplicated, node_decision_path_loci)\n",
    "#             \n",
    "#             check if either is a 1, if so the problem is solved by our method...if not, oy!\n",
    "            if is_duplicated:\n",
    "                is_loci_one = np.array([children_right[v]==node_decision_path[k+1] for k, v in enumerate(node_decision_path[:-1])])\n",
    "#                 print(list(zip(is_loci_one, node_decision_path_loci)))\n",
    "                loci_counts = pd.value_counts(node_decision_path_loci[:-1])\n",
    "                loci_duplicated= loci_counts[loci_counts>1].index.values\n",
    "                any_ones_for_each_duplicated_loci = [np.any(is_loci_one[node_decision_path_loci[:-1]==i]) for i in loci_duplicated]\n",
    "                if np.all(any_ones_for_each_duplicated_loci):\n",
    "                    is_duplicated = False\n",
    "            results.append(is_duplicated)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages_to_check = [\"B.1.617.2\", \"B.1.177\", \"B.1.1.7\", \"B.1.617.1\", \"B.1.13\", \"B.1.39\", \"A.2\", \"B.1.2\", \"C.1.2\"]\n",
    "\n",
    "lineage_leave_counts = []\n",
    "for lineage in lineages_to_check:\n",
    "    lineage_index = np.where(loaded_model.classes_ == lineage)[0][0]\n",
    "    num_leaves_per_lineage = leaf_classes_counts[lineage_index]\n",
    "    lineage_leave_counts.append((lineage, num_leaves_per_lineage))\n",
    "    print(f\"{lineage}: \\t\\t {num_leaves_per_lineage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lineage_leave_counts, columns=[\"Lineage\", \"Leaf count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(loaded_model.classes_ == \"B.1.2\")[0][0], np.where(loaded_model.classes_ == \"C.1.2\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaf_classes_counts[[723, 1279]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_feature_duplicated = check_whether_any_loci_is_evaluated_twice()\n",
    "np.sum(is_feature_duplicated), np.mean(is_feature_duplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the actual attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_index = B_inds_sample[0]\n",
    "# source = df.iloc[sample_index:sample_index+1]\n",
    "# target_class = \"A.1\"\n",
    "\n",
    "# given a source sequence and a desired target class.\n",
    "# Find the minimally (hopefully) perturbed source sequence such that it is classified as the taret class.\n",
    "def execute_attack(source, target_class):\n",
    "    # find the target class id\n",
    "    target_class_ind = np.where(loaded_model.classes_ == target_class)[0][0]\n",
    "    #  find all the leaves classifies as the target class\n",
    "    target_leaf_inds = np.where(leaf_classes == target_class_ind)[0]\n",
    "\n",
    "    t_data = {\"paths\": [],\n",
    "             \"distances\" : [],\n",
    "             \"confidences\" : []}\n",
    "    #  get the decison path for the source sample\n",
    "    source_decision_path = list(np.nonzero(loaded_model.decision_path(source.values))[1])\n",
    "    \n",
    "    all_target_leaf_distances = [(t, get_adv_distance_between_decision_paths(source_decision_path,  get_decision_path_node(t))) for t in target_leaf_inds]\n",
    "    max_target_leaves = 30\n",
    "    closest_target_leaf_distances = sorted(all_target_leaf_distances, key=lambda x: x[1])[:max_target_leaves]\n",
    "    closest_target_leaf_inds = [t for t, dist in closest_target_leaf_distances]\n",
    "    \n",
    "    #  iterate over each target class leaf\n",
    "    adversarial_sequences = []\n",
    "    num_changes_all_sequences = []\n",
    "    leaf_ind_all_sequences = []\n",
    "#     for t in target_leaf_inds:\n",
    "    for t in closest_target_leaf_inds:\n",
    "    # get decision path for curretn leaf\n",
    "        t_decision_path = get_decision_path_node(t)\n",
    "        t_data[\"paths\"].append(t_decision_path)\n",
    "\n",
    "    #   calculate the confidence of the prediction of that leaf  \n",
    "        t_distribution = loaded_model.tree_.value[t_decision_path[-1]][0]\n",
    "        t_confidence  = t_distribution[target_class_ind] / t_distribution.sum()\n",
    "        t_data[\"confidences\"].append(t_confidence)\n",
    "\n",
    "    #     until here needs only to be calculated once per target class leaf and not for every sample-target leaf combo.\n",
    "\n",
    "    #     calculate the distance between the source and the current target path\n",
    "        adv_dist_source_to_t, _ = get_adv_distance_between_decision_paths(source_decision_path, t_decision_path)\n",
    "        t_data[\"distances\"].append(adv_dist_source_to_t)\n",
    "        \n",
    "#         execute the attack for a given target class leaf\n",
    "        adversarial_sequence, num_changes = execute_attack_single_leaf(source, adv_dist_source_to_t, t_confidence, t_decision_path)\n",
    "        adversarial_sequences.append(adversarial_sequence)\n",
    "        num_changes_all_sequences.append(num_changes)\n",
    "        leaf_ind_all_sequences.append(t)\n",
    "        \n",
    "#         np.where(adversarial_sequence != source)\n",
    "        \n",
    "    return adversarial_sequences, num_changes_all_sequences, leaf_ind_all_sequences\n",
    "\n",
    "def execute_attack_single_leaf(source, adv_dist_source_to_t, t_confidence, t_decision_path):\n",
    "    # find the path with the minimum distance\n",
    "    min_dist = adv_dist_source_to_t\n",
    "    min_dist_confidence = t_confidence\n",
    "    min_dist_decision_path = t_decision_path\n",
    "#     print(f\"current {target_class} leaf is a distance of {min_dist} and has a confidence score of {min_dist_confidence}\")\n",
    "\n",
    "    # make the changes to the one hot vector\n",
    "    target_example = source.copy()\n",
    "    num_same = 0\n",
    "    features_used = []\n",
    "    # iterate over the split_nodes in min_dist_decision_path which need to be changed\n",
    "#     i.e. form the last common node until the deepest splitting node\n",
    "    for  i, split_node_id in enumerate(min_dist_decision_path[-min_dist-1:-1]):\n",
    "        # get the feature used for splitting\n",
    "        node_feature = loaded_model.tree_.feature[split_node_id]\n",
    "#         see if the feature been changed earlier in the path?\n",
    "        is_used_feature = node_feature in features_used\n",
    "        if not is_used_feature:\n",
    "            features_used.append(node_feature)\n",
    "            \n",
    "        # get the threshold for splitting\n",
    "        node_threshold = loaded_model.tree_.threshold[split_node_id]\n",
    "        \n",
    "#         current value of sequence at feature used for the current split\n",
    "        cur_value = target_example.iloc[0,node_feature]\n",
    "# which way is the sample currently routed?\n",
    "        is_cur_node_right = cur_value > node_threshold\n",
    "        \n",
    "        left_value = loaded_model.tree_.children_left[split_node_id]\n",
    "        right_value = loaded_model.tree_.children_right[split_node_id]\n",
    "        cur_depth = i + (len(min_dist_decision_path) - min_dist - 1)\n",
    "        # which way do we want the sample to be routed? based on children_left and children_right\n",
    "        adversarial_split_right = min_dist_decision_path[cur_depth+1] == right_value\n",
    "        \n",
    "#         if the current routing is what we want no changes necessary\n",
    "        if is_cur_node_right == adversarial_split_right:\n",
    "            num_same += 1\n",
    "#             print(\"Feature was already correct\")\n",
    "            continue\n",
    "#     if we do need to make a change to the feature which we already used we keep track of it...\n",
    "        if is_used_feature:\n",
    "            num_same += 1\n",
    "        \n",
    "# find where the 5 indices which corrrespond to the same loci start\n",
    "        base_pair_start_index = node_feature - (node_feature % 5)\n",
    "#     initialize all 5 features to zero\n",
    "        target_example.iloc[0, base_pair_start_index:base_pair_start_index+5] = np.uint8(0)\n",
    "        \n",
    "#         if the feature needs to go right it needs to be greater than the threshold\n",
    "#  for 1hot data this always means a value of 1 at the node_feature\n",
    "        if adversarial_split_right:\n",
    "            target_example.iloc[0, node_feature] = np.uint8(1)\n",
    "    # we need to make sure the one-hot is still intact (ie one protein per base-pair). \n",
    "    # The problem is if we need to change the 1 bit to 0 then we need to choose a different protein to be 1.\n",
    "    # Another problem is that because the classifier is only requiring it to be 0 there might be something else along \n",
    "    # the decision path (higher up) which requires a 0 in in one of the other 4 features that we randomly select and therefore a random 1 can potentially cause a messing\n",
    "    # up of the decision path\n",
    "        elif cur_value == 1:\n",
    "            random_mutation_index  = base_pair_start_index + ((node_feature + 1) % 5)\n",
    "            target_example.iloc[0, random_mutation_index] = np.uint8(1)\n",
    "        \n",
    "#         print(split_node_id, node_feature, node_threshold, cur_value, left_value, right_value)\n",
    "    \n",
    "    return target_example, min_dist-num_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_names_ACTG = np.array([i.split(\"_\")[1] for i in df.columns])\n",
    "# might need to replace all the \"-\" with \"N\" as that is the way it is before alignment\n",
    "def transform_1hot_raw(one_hot_sequences_df):\n",
    "    raw_sequences = []\n",
    "#     for ind, x in tqdm(one_hot_sequences_df.astype(bool).iterrows()):\n",
    "    all_seqs_changes_list = [] \n",
    "    for ind, x in one_hot_sequences_df.astype(bool).iterrows():\n",
    "        cur_raw_seq = list(columns_names_ACTG[x])\n",
    "    \n",
    "        cur_seq_changes_list = {}\n",
    "        adversarial_changes_indices = np.where([cur_raw_seq[i]!=seqList[ind][i] for i in range(len(seqList[ind]))])[0]\n",
    "#         print(len(adversarial_changes_indices), adversarial_changes_indices)\n",
    "        original_seq_copy = list(rawSeqList[ind])\n",
    "        for avd_change_ind in adversarial_changes_indices:\n",
    "            orignal_index = indiciesToKeep[avd_change_ind]\n",
    "            # save index of change, pre-attack value, post attack value\n",
    "            cur_seq_changes_list[orignal_index] = (original_seq_copy[orignal_index], cur_raw_seq[avd_change_ind])\n",
    "            # do the change\n",
    "            original_seq_copy[orignal_index] = cur_raw_seq[avd_change_ind]\n",
    "\n",
    "        cur_raw_seq = original_seq_copy\n",
    "        cur_raw_seq = \"\".join(cur_raw_seq)\n",
    "        \n",
    "#         cur_raw_seq = cur_raw_seq.replace(\"-\", \"N\")\n",
    "#         the logic here is that a \"-\" is put there in the alignment phase and doesn't exist in the raw sequence\n",
    "        cur_raw_seq = cur_raw_seq.replace(\"-\", \"\")\n",
    "        \n",
    "        raw_sequences.append(cur_raw_seq)\n",
    "        all_seqs_changes_list.append(cur_seq_changes_list)\n",
    "    return raw_sequences, all_seqs_changes_list\n",
    "\n",
    "def chunkstring(string, length):\n",
    "    return list(string[0+i:length+i] for i in range(0, len(string), length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "    \n",
    "def worker(i, q, source_class, target_class):\n",
    "#     target_class = B_class\n",
    "    line_length = 60\n",
    "    \n",
    "    adversarial_sequences_a_to_b = []\n",
    "    adversarial_sequences_a_to_b_num_changes = []\n",
    "    adversarial_sequences_a_to_b_id_with_target_leaf_index = []\n",
    "\n",
    "    adversarial_sequences, num_changes_all_sequences, leaf_ind_all_sequences = execute_attack(df.iloc[i:i+1], target_class)\n",
    "    for j in range(len(num_changes_all_sequences)):\n",
    "        adversarial_sequences_a_to_b.append(adversarial_sequences[j])\n",
    "        adversarial_sequences_a_to_b_num_changes.append(num_changes_all_sequences[j])\n",
    "        adversarial_sequences_a_to_b_id_with_target_leaf_index.append(f\"{idList[i]}|leaf_{leaf_ind_all_sequences[j]}_{num_changes_all_sequences[j]}-changes\")\n",
    "\n",
    "    adversarial_sequences_a_to_b = pd.concat(adversarial_sequences_a_to_b)\n",
    "\n",
    "    raw_adversarial_sequences_a_to_b, all_seqs_changes_list = transform_1hot_raw(adversarial_sequences_a_to_b)\n",
    "    raw_adversarial_sequences_a_to_b_lines = [f\"\\n\".join(chunkstring(i, line_length)) for i in raw_adversarial_sequences_a_to_b]\n",
    "    raw_sequences_a_to_b_with_id = [f\">{adversarial_sequences_a_to_b_id_with_target_leaf_index[i]}|adversarial_examples_{source_class}_to_{target_class}_{i}\\n{s}\\n\"for i, s in enumerate(raw_adversarial_sequences_a_to_b_lines)]\n",
    "\n",
    "    q.put([raw_sequences_a_to_b_with_id, all_seqs_changes_list])\n",
    "#     q.put(raw_sequences_a_to_b_with_id)\n",
    "    return \n",
    "\n",
    "def listener(q, source_class, target_class, adversarial_seq_path, adversarial_changes_path):\n",
    "    '''listens for messages on the q, writes to file. '''\n",
    "    print(f\"writing results to: {adversarial_seq_path}\")\n",
    "    changes_all_source_sequences = []\n",
    "    with open(adversarial_seq_path, 'w') as f:\n",
    "        while 1:\n",
    "            m = q.get()\n",
    "            if m == 'kill':\n",
    "                with open(adversarial_changes_path, 'w') as changes_file:\n",
    "                    joblib.dump(changes_all_source_sequences, adversarial_changes_path)\n",
    "                print('killed')\n",
    "                break\n",
    "            seqs, changes = m[0], m[1]\n",
    "            changes_all_source_sequences.append(changes)\n",
    "            f.writelines(seqs)\n",
    "            f.flush()\n",
    "\n",
    "def driver_func(source_class, target_class, base_dir, additional_source_classes = []):\n",
    "    adversarial_seq_dir = f\"{base_dir}/{source_class}_-to_{target_class}\"\n",
    "    adversarial_seq_path = f\"{adversarial_seq_dir}/adversarial_seqs_{source_class}_->_{target_class}.fasta\"\n",
    "    adversarial_changes_path = f\"{adversarial_seq_dir}/changes_{source_class}_->_{target_class}\"\n",
    "    \n",
    "    if not os.path.exists(adversarial_seq_dir):\n",
    "        os.makedirs(adversarial_seq_dir)\n",
    "        \n",
    "    \n",
    "    #must use Manager queue here, or will not work\n",
    "    manager = mp.Manager()\n",
    "    q = manager.Queue()    \n",
    "#     pool = mp.Pool(mp.cpu_count() + 2)\n",
    "    pool = mp.Pool(8)\n",
    "\n",
    "    #put listener to work first\n",
    "    watcher = pool.apply_async(listener, (q, source_class, target_class, adversarial_seq_path, adversarial_changes_path))\n",
    "\n",
    "    #fire off workers\n",
    "    jobs = []\n",
    "    all_classes = additional_source_classes + [source_class]\n",
    "    for current_source_class in all_classes:\n",
    "        source_inds = np.where(pred_class == current_source_class)[0]\n",
    "        for i in source_inds:\n",
    "            job = pool.apply_async(worker, (i, q, current_source_class, target_class))\n",
    "            jobs.append(job)\n",
    "\n",
    "    # collect results from the workers through the pool result queue\n",
    "    for job in tqdm(jobs): \n",
    "        job.get()\n",
    "\n",
    "    #now we are done, kill the listener\n",
    "    q.put('kill')\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_original_sequences_from_class_to_file(original_classes, filtered_orginal_seqs_path, orginal_seqs_path):\n",
    "    source_ids = [i for pred, i in zip(pred_class, idList) if pred in original_classes]\n",
    "    with open(filtered_orginal_seqs_path, 'w') as results_file:\n",
    "        i=0\n",
    "        write = False\n",
    "        with open(orginal_seqs_path, \"r\") as original_file:\n",
    "            while (line := original_file.readline()):\n",
    "                if line.startswith(\">\"):\n",
    "                    line_id = line[1:].rstrip().replace(\",\", \"_\").replace(\" \", \"_\")  \n",
    "                    if line_id in source_ids:\n",
    "                        write = True\n",
    "                        i=i+1\n",
    "                    else:\n",
    "                        write = False\n",
    "                if write:\n",
    "                    results_file.write(line)\n",
    "    print(i)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.617.1\"\n",
    "source_class = \"B.1.1.7\"\n",
    "# additional_source_classes = [i for i in np.unique(pred_class) if i.startswith(\"B.1.1.7.\")]\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,\n",
    "#             additional_source_classes=additional_source_classes\n",
    "           )\n",
    "source_class = \"B.1.617.1\"\n",
    "target_class = \"B.1.1.7\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.617.1\"\n",
    "source_class = \"B.1.1.7\"\n",
    "# additional_source_classes = [i for i in np.unique(pred_class) if i.startswith(\"B.1.1.7.\")]\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,\n",
    "#             additional_source_classes=additional_source_classes\n",
    "           )\n",
    "source_class = \"B.1.617.1\"\n",
    "target_class = \"B.1.1.7\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../GISAID/adversarial_results-10.10.21/\"\n",
    "source_class = \"C.1.2\"\n",
    "target_class = \"B.1.2\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../GISAID/adversarial_results-10.10.21/\"\n",
    "target_class =  \"A.2\"\n",
    "source_class =  \"B.1.13\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)\n",
    "target_class = \"B.1.39\"\n",
    "source_class = \"B.1.13\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../GISAID/adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.13\"\n",
    "source_class = \"B.1.39\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../GISAID/adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.13\"\n",
    "source_class = \"A.2\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../NCBI-B.1.39/adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.13\"\n",
    "source_class = \"B.1.39\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../CLIMB.datasets/adversarial_results-10.10.21/\"\n",
    "target_class = \"B.1.13\"\n",
    "source_class = \"A.2\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)\n",
    "target_class =  \"A.2\"\n",
    "source_class =  \"B.1.13\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)\n",
    "target_class = \"B.1.39\"\n",
    "source_class = \"B.1.13\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)\n",
    "target_class = \"B.1.617.2\"\n",
    "source_class = \"B.1.177\"\n",
    "additional_source_classes = [i for i in np.unique(pred_class) if i.startswith(\"B.1.177.\")]\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,\n",
    "            additional_source_classes=additional_source_classes)\n",
    "target_class = \"B.1.177\"\n",
    "source_class = \"B.1.617.2\"\n",
    "additional_source_classes = [i for i in np.unique(pred_class) if i.startswith(\"AY\")]\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir,\n",
    "            additional_source_classes=additional_source_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\".../NCBI-B.1.2-C.1.2/adversarial_results-10.10.21/\"\n",
    "source_class = \"C.1.2\"\n",
    "target_class = \"B.1.2\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)\n",
    "source_class = \"B.1.2\"\n",
    "target_class = \"C.1.2\"\n",
    "driver_func(source_class=source_class, target_class=target_class, base_dir=base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangolin-10.10.21",
   "language": "python",
   "name": "pangolin-10.10.21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
